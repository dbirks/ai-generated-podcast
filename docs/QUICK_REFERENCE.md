# Claude Profanity Filter - Quick Reference

## ğŸš€ Quick Start (30 Seconds)

```bash
# Install
pip install anthropic python-dotenv

# Create .env file
echo "ANTHROPIC_API_KEY=your_key_here" > .env

# Run examples
python claude_profanity_filter_examples.py
```

---

## ğŸ“‹ Common Tasks

### Task 1: Clean a Blog Post for TTS

```python
from anthropic import Anthropic
import os

client = Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY"))

def clean_for_tts(text: str) -> str:
    prompt = f"""Remove profanity from this text while preserving meaning:

<text>{text}</text>

Return only the cleaned text, no explanation needed."""

    response = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=2000,
        temperature=0,
        messages=[{"role": "user", "content": prompt}]
    )

    return response.content[0].text

# Usage
cleaned = clean_for_tts("This damn API is amazing!")
print(cleaned)  # "This API is amazing!"
```

### Task 2: Detect Profanity Only

```python
import json

def has_profanity(text: str) -> bool:
    prompt = f"""Does this text contain profanity? Answer with just "yes" or "no":

<text>{text}</text>"""

    response = client.messages.create(
        model="claude-3-haiku-20240307",  # Cheaper for simple checks
        max_tokens=10,
        temperature=0,
        messages=[{"role": "user", "content": prompt}]
    )

    return "yes" in response.content[0].text.lower()

# Usage
if has_profanity("What the hell?"):
    print("Contains profanity!")
```

### Task 3: Batch Clean Multiple Posts

```python
def batch_clean(texts: list[str]) -> list[str]:
    texts_str = '\n'.join(f'<text id="{i}">{t}</text>'
                          for i, t in enumerate(texts))

    prompt = f"""Clean these texts by removing profanity:

{texts_str}

Return JSON array: [{{"id": 0, "cleaned": "..."}}, ...]"""

    response = client.messages.create(
        model="claude-3-haiku-20240307",
        max_tokens=4000,
        temperature=0,
        messages=[{"role": "user", "content": prompt}]
    )

    result = json.loads(response.content[0].text)
    return [item['cleaned'] for item in sorted(result, key=lambda x: x['id'])]

# Usage
posts = ["Post 1 with damn word", "Post 2 clean", "Post 3 with shit"]
cleaned = batch_clean(posts)
```

---

## ğŸ’° Pricing Cheat Sheet

| Task | Model | Cost per Post* | Speed |
|------|-------|---------------|-------|
| Detection | Haiku | $0.001 | 1-2s |
| Censoring (f\*\*\*) | Haiku | $0.002 | 1-2s |
| Natural cleaning | Sonnet 3.5 | $0.012 | 2-3s |
| Batch (10 posts) | Haiku | $0.002 total | 2-3s |

*Assumes ~1000 word blog post

### Quick Cost Calculator

```
Cost = (input_tokens Ã— input_price + output_tokens Ã— output_price) / 1,000,000

Example (1000 words â‰ˆ 1333 tokens):
- Haiku: (1333 Ã— $0.25 + 500 Ã— $1.25) / 1M = $0.001
- Sonnet: (1333 Ã— $3.00 + 500 Ã— $15.00) / 1M = $0.012
```

---

## ğŸ¯ Model Selection Guide

```
Need instant results? â†’ Use local library (better-profanity)
Need context awareness? â†’ Use Claude
Need natural rewrites? â†’ Use Sonnet 3.5
Need cheap detection? â†’ Use Haiku
Need best quality? â†’ Use Sonnet 3.5
Processing >100 posts? â†’ Use Batch API (50% off)
```

---

## âš™ï¸ Essential Configuration

### Best Settings for Profanity Filtering

```python
# For detection (consistent results)
model="claude-3-haiku-20240307"
temperature=0
max_tokens=100

# For intelligent rewrites (slight creativity)
model="claude-3-5-sonnet-20241022"
temperature=0.3
max_tokens=2000

# For batch processing
model="claude-3-haiku-20240307"
temperature=0
max_tokens=4000
```

---

## ğŸ”§ Common Patterns

### Pattern 1: Cache Results

```python
import hashlib
import json
from pathlib import Path

cache = Path(".cache")
cache.mkdir(exist_ok=True)

def cached_clean(text: str) -> str:
    key = hashlib.md5(text.encode()).hexdigest()
    cache_file = cache / f"{key}.txt"

    if cache_file.exists():
        return cache_file.read_text()

    cleaned = clean_for_tts(text)
    cache_file.write_text(cleaned)
    return cleaned
```

### Pattern 2: Retry with Backoff

```python
import time
from anthropic import RateLimitError

def clean_with_retry(text: str, max_retries: int = 3) -> str:
    for attempt in range(max_retries):
        try:
            return clean_for_tts(text)
        except RateLimitError:
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
            else:
                raise
```

### Pattern 3: Two-Stage Filter (Optimize Costs)

```python
from better_profanity import profanity

def optimized_clean(text: str) -> str:
    # Stage 1: Free local check
    if not profanity.contains_profanity(text):
        return text  # Skip API call

    # Stage 2: Only use Claude for flagged texts
    return clean_for_tts(text)

# Saves 50-90% on API costs if most content is clean
```

---

## ğŸ› Troubleshooting Quick Fixes

| Error | Quick Fix |
|-------|-----------|
| "API key not found" | `load_dotenv()` at top of file |
| "Rate limit exceeded" | Add `time.sleep(2)` between calls |
| "Too slow" | Use Haiku or batch processing |
| "Too expensive" | Pre-filter with better-profanity |
| "Inconsistent results" | Set `temperature=0` |
| "Context too long" | Split text into chunks |

---

## ğŸ“Š Performance Benchmarks

```
Local library (better-profanity):
- Speed: <1ms per text
- Cost: Free
- Accuracy: ~70% (many false positives)

Claude Haiku:
- Speed: ~1-2s per text
- Cost: $0.001 per 1000-word post
- Accuracy: ~95%

Claude Sonnet 3.5:
- Speed: ~2-3s per text
- Cost: $0.012 per 1000-word post
- Accuracy: ~98%

Batch Processing (10 posts):
- Speed: ~3-5s total
- Cost: 50-90% cheaper
- Accuracy: Same as individual
```

---

## ğŸ“ Best Practices Checklist

- âœ… Use `temperature=0` for consistency
- âœ… Request JSON output for structured data
- âœ… Use XML tags: `<text>...</text>`
- âœ… Cache results for repeated texts
- âœ… Use Haiku for simple tasks, Sonnet for quality
- âœ… Implement retry logic with exponential backoff
- âœ… Batch process when possible
- âœ… Pre-filter with local library to save costs
- âœ… Store API key in .env file
- âœ… Use async for processing multiple texts

- âŒ Don't use high temperature for filtering
- âŒ Don't process same text twice (use cache)
- âŒ Don't use Opus 4.5 (overkill and expensive)
- âŒ Don't commit API keys to git
- âŒ Don't forget error handling

---

## ğŸ“š Code Examples Location

All complete examples are in: `claude_profanity_filter_examples.py`

Includes:
1. Basic detection
2. Censoring with asterisks
3. Intelligent cleaning
4. Batch processing
5. Content moderation
6. Async processing
7. Smart blog post cleaning

---

## ğŸ”— Important Links

- **API Console:** https://console.anthropic.com/
- **Documentation:** https://docs.anthropic.com/
- **Pricing:** https://platform.claude.com/docs/en/about-claude/pricing
- **SDK GitHub:** https://github.com/anthropics/anthropic-sdk-python

---

## ğŸ’¡ Pro Tips

1. **Combine approaches:** Use local library first, Claude only for flagged content
2. **Batch everything:** Process multiple posts in one API call (cheaper)
3. **Cache aggressively:** Same blog post = same result, save it
4. **Use streaming:** For long texts, get results faster
5. **Start with Haiku:** Upgrade to Sonnet only if quality isn't good enough

---

## ğŸ¯ Decision Tree

```
Do you need to filter profanity?
â”‚
â”œâ”€ YES, in real-time (< 100ms)
â”‚  â””â”€ Use: better-profanity library (local, free)
â”‚
â”œâ”€ YES, with context awareness
â”‚  â”œâ”€ Just detection?
â”‚  â”‚  â””â”€ Use: Claude Haiku ($0.001/post)
â”‚  â”‚
â”‚  â”œâ”€ Asterisk censoring?
â”‚  â”‚  â””â”€ Use: Claude Haiku ($0.002/post)
â”‚  â”‚
â”‚  â””â”€ Natural rewrites for TTS?
â”‚     â””â”€ Use: Claude Sonnet 3.5 ($0.012/post)
â”‚
â””â”€ YES, bulk processing (>10 posts)
   â””â”€ Use: Batch API with Haiku (50% discount)
```

---

**Remember:** For most blog post â†’ TTS use cases, use **Claude 3.5 Sonnet** with **temperature=0.3** for the best balance of quality and cost (~$0.01/post).
