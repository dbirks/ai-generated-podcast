# AGENTS.md

Instructions for Claude Code when working with this repository.

## Quick Episode Workflow

When user pastes a blog URL, follow these steps:

```bash
# 1. Scrape article (works for most sites, not Medium)
uv run main.py scrape https://example.com/post

# 2. Check for profanity (f-word, s-word, d-word - NOT fart/ass/crap)
grep -i -E "fuck|shit|damn|bitch|bastard" temp/article.txt

# 3. If profanity found, manually edit with Edit tool:
#    fuck -> freaking, shit -> stuff, damn -> darn, etc.

# 4. Generate TTS (auto-chunks, uses cedar voice by default)
uv run main.py tts temp/article.txt -o "temp/Episode Title.mp3"

# 4a. Optional: Use intro feature (Marin voice for intro, Cedar for main)
uv run main.py tts temp/article.txt -o "temp/Episode Title.mp3" \
  --intro "Welcome to the AI Generated Podcast. This episode covers..."

# 5. Upload to Azure
uv run main.py upload "temp/Episode Title.mp3" --name "Episode Title.m4a"

# 6. Add to episodes.yaml (scrape command prints metadata)
# 7. Regenerate feed and push
uv run main.py feed && git add -A && git commit -m "Add Episode Title" && git push
```

## Medium Articles (Cloudflare blocked)

Medium blocks curl. Use Wayback Machine:
```bash
curl -sL "https://web.archive.org/web/https://medium.com/..." -o /tmp/article.html
uv run python3 -c "from scraper import ...; # extract with readability"
```

Or ask user to paste text manually.

## TTS Providers

**OpenAI (default):**
- Model: `gpt-4o-mini-tts` (newest, best quality)
- Voice: `cedar` (newest, most natural)
- Other voices: marin, nova, alloy, ash, coral, echo, fable, onyx, sage, shimmer
- Chunks at 4k chars, auto-concatenates with ffmpeg

**ElevenLabs (fallback):**
- Use `-p elevenlabs` flag
- Chunks at 10k chars
- Quota-based, can run out

**Intro Feature (Multi-Voice):**
- Use `--intro "intro text"` to add a separate intro with different voice
- Intro voice: `marin` (default, configurable with `--intro-voice`)
- Main voice: `cedar` (default, configurable with `--main-voice`)
- Auto-adds 2s pause between intro and main (configurable with `--pause`)
- Example: `uv run main.py tts article.txt -o output.mp3 --intro "Welcome to the show"`
- Works with both OpenAI and ElevenLabs providers

## Profanity Rules

Light touch - only edit strong profanity:
- fuck/fucking -> freaking
- shit -> stuff
- shitbag -> jerk
- damn/damnedest -> darn/darndest
- bitch/bastard -> jerk

Keep these (mild): fart, ass, crap, hell

## Architecture

```
main.py       - Typer CLI (scrape, tts, upload, feed, list)
scraper.py    - Article extraction with readability-lxml
tts.py        - OpenAI/ElevenLabs TTS with auto-chunking
storage.py    - Azure Blob Storage upload
feed.py       - RSS generation with feedgen (iTunes/Spotify compatible)
episodes.yaml - Episode metadata
```

## Episode Title Prefixes

All episode titles should be prefixed with a bracket tag indicating the source type:

- `[Blog]` - Articles from blog posts (most episodes)
- `[NotebookLM]` - Content generated by Google's NotebookLM
- `[Tidbit]` or `[Short]` - AI-researched short-form content (future)

**Examples:**
- `[Blog] Zero Framework Cognition`
- `[NotebookLM] TLS 1.3 Perfect Forward Secrecy`
- `[Tidbit] Understanding OAuth Resource Servers` (future)

This helps users quickly identify the episode source in podcast apps.

## Episode Schema

```yaml
- title: "[Blog] Episode Title"  # Include source prefix
  published_date: "2026-01-08T12:00:00-05:00"
  blog_url: https://example.com/post
  was_edited: false  # true if profanity cleaned
  author: Author Name
  article_date: "2026-01-08"
  tech: Claude, OpenAI TTS (cedar)
  description: |
    2-4 line summary of the episode content.
```

## Automated Episode Generation (GitHub Actions)

Episodes can be generated automatically from GitHub issues:

### Creating an Episode via Issue

1. Create a new issue with the `podcast-episode` label OR title starting with `[Episode]`
2. Include the blog URL in the issue body (required)
3. Optional metadata:
   ```
   URL: https://example.com/article
   Title: Custom Episode Title
   Author: Author Name
   Description: Episode summary
   ```

The workflow will:
1. Extract URL and metadata from issue
2. Scrape article
3. Check for profanity (stops if found - requires manual edit)
4. Generate TTS with Cedar voice
5. Upload to Azure
6. Add to episodes.yaml
7. Regenerate RSS feed
8. Commit and push changes
9. Comment progress updates on issue
10. Close issue on success

**Required Repository Secrets:**
- `OPENAI_API_KEY` - For TTS generation
- `AZURE_STORAGE_CONNECTION_STRING` - For audio upload

**Workflow file:** `.github/workflows/generate-episode-from-issue.yml`

**Manual trigger:** Can also run via workflow_dispatch for specific issue numbers

### Example Issue

```markdown
Title: [Episode] New Article About AI
Labels: podcast-episode

URL: https://example.com/article-about-ai
Author: John Doe
Description: A deep dive into modern AI techniques
```

## Deployment

GitHub Actions on push to main:
1. `uv run main.py feed` generates rss.xml
2. Uploads only rss.xml, logo.png, index.html to Pages (not temp/)
3. ~30s deploy time

## Batch Processing

When processing multiple articles, use parallel Task agents:
- Scrape all articles in parallel
- Check profanity in parallel
- Run TTS in parallel background agents
- Upload to Azure in parallel
- Add all to episodes.yaml at once

This lets the user queue work while processing continues.

## External URLs

- RSS: https://dbirks.github.io/ai-generated-podcast/rss.xml
- Audio: https://birkspublic.blob.core.windows.net/aigeneratedpodcast/
- Spotify: https://open.spotify.com/show/7ChYkvtx2lftMXIaouaIKN
